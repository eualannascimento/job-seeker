{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\### Project status <br>\n",
    "Obs: Notebook criado para o projeto job-seeker, será utilizado até as funcionalidade abaixo estarem em produção\n",
    "\n",
    "- [ ] Transpor dataframe para o Notion\n",
    "- [ ] Fazer todas as chamadas aos sites de recrutamento via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libs and clear output\n",
    "from IPython.display import clear_output\n",
    "%pip install pytz datetime requests openpyxl pandas bs4 unidecode\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pandas FutureWarnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pytz\n",
    "import datetime\n",
    "import requests\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Import my libs\n",
    "import modules.filters as filters\n",
    "import modules.handler as handler\n",
    "import modules.notion as notion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get html from website through webscraping\n",
    "def website_request(url, site_type):\n",
    "    if(site_type == 'Workable'):\n",
    "        #POST request\n",
    "        page = requests.post(url)\n",
    "        data = page.json()\n",
    "    else:\n",
    "        #GET request\n",
    "        page = requests.get(url)\n",
    "        data = BeautifulSoup(page.content, \"html.parser\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import companies dataframe and adjust some URLs for requests\n",
    "company_df = pd.read_csv('./companies.csv')\n",
    "company_df.loc[company_df['Site'].str[-1] == '/', 'Site'] = company_df['Site'].str[:-1]\n",
    "company_df.loc[company_df['Tipo de Site'] == 'Kenoby', 'Site'] = company_df['Site'] + '/position'\n",
    "company_df.loc[company_df['Tipo de Site'] == 'Workable', 'Site'] = company_df['Site'].apply(lambda x: x.split('/')[-1])\n",
    "company_df.loc[company_df['Tipo de Site'] == 'Workable', 'Site'] = 'https://apply.workable.com/api/v3/accounts/' + company_df['Site'] + '/jobs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get jobs\n",
    "def get_array_of_jobs(response, company_name, company_type, site_type, site_url):\n",
    "    results = []\n",
    "    if (site_type == 'Gupy'): results = handler.treat_gupy(response, company_name, company_type, site_type, site_url)\n",
    "    if (site_type == 'Lever'): results = handler.treat_lever(response, company_name, company_type, site_type, site_url)\n",
    "    if (site_type == 'Greenhouse'): results = handler.treat_greenhouse(response, company_name, company_type, site_type, site_url)\n",
    "    if (site_type == 'Kenoby'): results = handler.treat_kenoby(response, company_name, company_type, site_type, site_url)\n",
    "    if (site_type == 'Workable'): results = handler.treat_workable(response, company_name, company_type, site_type, site_url)\n",
    "    return pd.DataFrame(results)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start collecting jobs\n",
    "full_job_df = pd.DataFrame()\n",
    "for n in company_df.index:\n",
    "    website_response = website_request(company_df['Site'][n], company_df['Tipo de Site'][n])\n",
    "    jobs = get_array_of_jobs(website_response, company_df['Empresa'][n], company_df['Tipo de Empresa'][n], company_df['Tipo de Site'][n], company_df['Site'][n])\n",
    "    full_job_df = pd.concat([full_job_df, jobs], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust titles (remove trailing spaces, underlines, etc)\n",
    "full_job_df['title'] = full_job_df['title'].str.replace('_', ' ')\n",
    "full_job_df['title'] = full_job_df['title'].str.replace('  ', ' ')\n",
    "full_job_df['title'] = [re.sub(r\"(?<=\\()\\s+|\\s+(?=\\))\", \"\", str(x)) for x in full_job_df['title']]\n",
    "full_job_df['title'] = full_job_df['title'].str.strip()\n",
    "full_job_df[(full_job_df['company'] == \"Ambev\") & (full_job_df['location'] == \"PELOTAS/RS\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to filter dict\n",
    "def filter_dict(df, search_column, filter_dict, new_column_name):\n",
    "    for key in filter_dict.keys():\n",
    "        key_filter = [unidecode(filter) for filter in filter_dict[key]]\n",
    "        regex = r'\\b(?:{})\\b'.format('|'.join(key_filter))\n",
    "        df.loc[df[search_column].apply(unidecode).str.contains(regex, na=False, case=False), new_column_name] = key\n",
    "        df.loc[df[new_column_name].isnull(), new_column_name] = \"99 - Não categorizado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter dict\n",
    "filter_dict(full_job_df, 'title', filters.dict_category, 'category')\n",
    "filter_dict(full_job_df, 'title', filters.dict_level, 'level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get now date/time and export dataframe to excel file\n",
    "now_sao_paulo = pytz.timezone('America/Sao_Paulo').localize(datetime.datetime.now())\n",
    "now_for_filename = now_sao_paulo.strftime(\"%Y_%m_%d-%H%M%S\")\n",
    "full_job_df.to_excel(rf'./results/xlsx/jobs_{now_for_filename}.xlsx', encoding='utf-8', index=False)\n",
    "#full_job_df.to_json('src/results/json/jobs.json', force_ascii=False, orient = 'records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_job-seeker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cb149e9d0037bd48ef9cdbd3ae63a4ef31b1d0d5484b079a170d54bfa9386fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
